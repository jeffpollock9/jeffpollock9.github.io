---
layout: post
title: "Multivariate normal covariance matrices and the cholesky decomposition"
permalink: /:title/
tags: [multivariate normal, cholesky]
---

<p>
This post is mainly some notes about linear algebra, the cholesky decomposition,
and a way of parametrising the multivariate normal which might be more efficient
in some cases. As you can see at the end, I am pretty sure I am missing at least
one important step right now.
</p>

<p>
The multivariate normal covariance matrix \(\Sigma\) is symmetric <a href="https://en.wikipedia.org/wiki/Positive-definite_matrix#Positive_semidefinite">positive
semi-definite</a> which means that it can be written as:
</p>

\begin{equation*}
\Sigma = L L^T
\end{equation*}

<p>
where \(L\) is lower triangular. This is known as the <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> and
is available in any half decent linear algebra library, for example
<a href="https://devdocs.io/numpy~1.14/generated/numpy.linalg.cholesky">numpy.linalg.cholesky</a> in python or <a href="https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/chol">chol</a> in R.
</p>

<p>
That means that one easy way to create a positive semi-definite matrix is to
start with \(L\):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> numpy <span style="color: #F0DFAF; font-weight: bold;">as</span> np

np.random.seed(42)

<span style="color: #DFAF8F;">L</span> = np.tril(np.random.normal(0.0, 1.0, [4, 4]))
<span style="color: #DFAF8F;">Sigma</span> = L @ L.T
<span style="color: #DFAF8F;">chol</span> = np.linalg.cholesky(Sigma)

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"L:\n{}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(L))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"chol:\n{}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(chol))
</pre>
</div>

<pre class="example">
L:
[[ 0.49671415  0.          0.          0.        ]
 [-0.23415337 -0.23413696  0.          0.        ]
 [-0.46947439  0.54256004 -0.46341769  0.        ]
 [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]]
chol:
[[ 0.49671415  0.          0.          0.        ]
 [-0.23415337  0.23413696  0.          0.        ]
 [-0.46947439 -0.54256004  0.46341769  0.        ]
 [ 0.24196227  1.91328024  1.72491783  0.56228753]]
</pre>

<p>
But we see here that <code>L</code> and <code>chol</code> are not the same - some of the signs have
switched. To ensure that this doesn't happen (i.e. there is a unique solution),
we need to ensure that the diagonal of \(L\) is positive (see the <a href="https://mc-stan.org/docs/2_18/reference-manual/covariance-matrices-1.html">covariance
matrices section of the stan manual</a>):
</p>

<div class="org-src-container">
<pre class="src src-python">np.random.seed(42)

<span style="color: #DFAF8F;">d</span> = 4
<span style="color: #DFAF8F;">L</span> = np.tril(np.random.normal(0.0, 1.0, [d, d]))
np.fill_diagonal(L, np.<span style="color: #DCDCCC; font-weight: bold;">abs</span>(np.diag(L)))
<span style="color: #DFAF8F;">Sigma</span> = L @ L.T
<span style="color: #DFAF8F;">chol</span> = np.linalg.cholesky(Sigma)

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"L:\n{}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(L))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"chol:\n{}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(chol))
</pre>
</div>

<pre class="example">
L:
[[ 0.49671415  0.          0.          0.        ]
 [-0.23415337  0.23413696  0.          0.        ]
 [-0.46947439  0.54256004  0.46341769  0.        ]
 [ 0.24196227 -1.91328024 -1.72491783  0.56228753]]
chol:
[[ 0.49671415  0.          0.          0.        ]
 [-0.23415337  0.23413696  0.          0.        ]
 [-0.46947439  0.54256004  0.46341769  0.        ]
 [ 0.24196227 -1.91328024 -1.72491783  0.56228753]]
</pre>

<p>
In addition to allowing us to easily create random covariance matrices, the
cholesky parametrisation of the multivariate normal PDF is much more
efficient. The typical PDF you see is:
</p>

\begin{equation*}
p(y | \mu, \Sigma) = \frac{1}{(2 \pi)^{d / 2} |\Sigma|^{1/2}} e^{-\frac{1}{2}(y - \mu)^T \Sigma^{-1} (y - \mu)}
\end{equation*}

<p>
where \(d\) is the dimension of the random vector. Implementing this directly
gives something like:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> scipy.stats <span style="color: #F0DFAF; font-weight: bold;">as</span> stats

<span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">multivariate_normal_pdf</span>(y, mu, Sigma):
    <span style="color: #DFAF8F;">d</span> = np.shape(mu)[0]
    <span style="color: #DFAF8F;">sqrt_det</span> = np.sqrt(np.linalg.det(Sigma))
    <span style="color: #DFAF8F;">two_pi_power</span> = np.power((2.0 * np.pi), d / 2.0)
    <span style="color: #DFAF8F;">y_minus_mu</span> = y - mu
    <span style="color: #DFAF8F;">exp</span> = np.exp(-0.5 * y_minus_mu.T @ np.linalg.inv(Sigma) @ y_minus_mu)
    <span style="color: #F0DFAF; font-weight: bold;">return</span> 1.0 / (two_pi_power * sqrt_det) * exp

<span style="color: #DFAF8F;">y</span> = np.random.normal(0.0, 0.1, [d, 1])
<span style="color: #DFAF8F;">mu</span> = np.random.normal(0.0, 0.1, [d, 1])

<span style="color: #DFAF8F;">me</span> = multivariate_normal_pdf(y, mu, Sigma)[0, 0]
<span style="color: #DFAF8F;">scipy</span> = stats.multivariate_normal.pdf(np.squeeze(y), np.squeeze(mu), Sigma)

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"me:    {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(me))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"scipy: {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(scipy))
</pre>
</div>

<pre class="example">
me:    0.10220544
scipy: 0.10220544

</pre>

<p>
However if we work with the cholesky parameterisation (i.e. use \(L\)) we
firstly have:
</p>

\begin{equation*}
|\Sigma| = |L L^T| = |L| |L^T| = |L|^2 = (\prod_{i=1}^{d} L_{i, i})^2
\end{equation*}

<p>
since for square matrices \(A\) and \(B\):
</p>

<ol class="org-ol">
<li>\(|A B| = |A| |B|\)</li>
<li>\(|A^T| = |A|\)</li>
<li>for a triangular matrix \(L\), the determinant is the product of the diagonal
entries</li>
</ol>

<p>
see <a href="https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant">properties of the determinant</a>. Additionally, the term in the PDF is
\(|\Sigma|^{1/2}\), so in the PDF we need only calculate \(|\Sigma|^{1/2} =
\prod_{i=1}^{d} L_{i, i}\). In code the square root of the determinant is:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">sqrt_Sigma_det</span> = np.sqrt(np.linalg.det(Sigma))
<span style="color: #DFAF8F;">sqrt_L_det</span> = np.prod(np.diag(L))

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"|Sigma|^0.5 = {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(sqrt_Sigma_det))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"|L|         = {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(sqrt_L_det))
</pre>
</div>

<pre class="example">
|Sigma|^0.5 = 0.03030453
|L|         = 0.03030453

</pre>

<p>
We secondly have:
</p>

\begin{equation*}
\Sigma^{-1} = (L L^T)^{-1} = L^{-T} L^{-1}
\end{equation*}

<p>
since for invertible square matrices \(A\) and \(B\): \((A B)^{-1} = B^{-1}
A^{-1}\) see <a href="https://en.wikipedia.org/wiki/Invertible_matrix#Other_properties">properties of invertible matrix</a>. Thus:
</p>

\begin{equation*}
(y - \mu)^T \Sigma^{-1} (y - \mu) = (y - \mu)^T L^{-T} L^{-1} (y - \mu) = (L^{-1}(y - \mu))^T L^{-1} (y - \mu)
\end{equation*}

<p>
since \((AB)^T = B^T A^T\) and \((A^T)^{-1} = (A^{-1})^T = A^{-T}\) - see
<a href="https://en.wikipedia.org/wiki/Transpose#Properties">properties of matrix transpose</a>. Considering just this part in code gives:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> scipy.linalg.lapack <span style="color: #F0DFAF; font-weight: bold;">as</span> lapack

<span style="color: #DFAF8F;">y_minus_mu</span> = y - mu

<span style="color: #DFAF8F;">L_inv</span>, <span style="color: #DFAF8F;">_</span> = lapack.dtrtri(L, lower=<span style="color: #BFEBBF;">True</span>)
<span style="color: #DFAF8F;">L_inv_y_minus_mu</span> = L_inv @ y_minus_mu
<span style="color: #DFAF8F;">L_exponent</span> = L_inv_y_minus_mu.T @ L_inv_y_minus_mu

<span style="color: #DFAF8F;">Sigma_exponent</span> = y_minus_mu.T @ np.linalg.inv(Sigma) @ y_minus_mu

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"Sigma exponent = {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(Sigma_exponent[0, 0]))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"L exponent     = {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(L_exponent[0, 0]))
</pre>
</div>

<pre class="example">
Sigma exponent = 4.20294853
L exponent     = 4.20294853

</pre>

<p>
I wasn't sure what the best way to exploit that \(L\) is lower triangular when
computing the inverse and had to use the low level lapack wrapper <code>trtri</code> (see the
<a href="https://software.intel.com/en-us/mkl-developer-reference-c-trtri">intel docs</a>) anyway, this inverse should be cheaper than inverting \(\Sigma\).
</p>

<p>
Putting it all together, a cholesky parametrised implementation might look
something like:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">multivariate_normal_cholesky_pdf</span>(y, mu, L):
    <span style="color: #DFAF8F;">d</span> = np.shape(mu)[0]
    <span style="color: #DFAF8F;">sqrt_det</span> = np.prod(np.diag(L))
    <span style="color: #DFAF8F;">two_pi_power</span> = np.power((2.0 * np.pi), d / 2.0)
    <span style="color: #DFAF8F;">L_inv</span>, <span style="color: #DFAF8F;">_</span> = lapack.dtrtri(L, lower=<span style="color: #BFEBBF;">True</span>)
    <span style="color: #DFAF8F;">L_inv_y_minus_mu</span> = L_inv @ (y - mu)
    <span style="color: #DFAF8F;">exp</span> = np.exp(-0.5 * (L_inv_y_minus_mu.T @ L_inv_y_minus_mu))
    <span style="color: #F0DFAF; font-weight: bold;">return</span> 1.0 / (two_pi_power * sqrt_det) * exp

<span style="color: #DFAF8F;">me_again</span> = multivariate_normal_cholesky_pdf(y, mu, L)[0, 0]

<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"me again:    {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(me_again))
<span style="color: #F0DFAF; font-weight: bold;">print</span>(<span style="color: #CC9393;">"scipy again: {:0.8f}"</span>.<span style="color: #DCDCCC; font-weight: bold;">format</span>(scipy))
</pre>
</div>

<pre class="example">
me again:    0.10220544
scipy again: 0.10220544

</pre>

<p>
After writing this (which is just a learning exercise) I had a look at the
<a href="https://pytorch.org/docs/stable/_modules/torch/distributions/multivariate_normal.html">pytorch implementation</a> which mentions something about the "squared Mahalanobis
distance". I guess that looks like another optimisation you can make which calls
the lapack function <code>trtrs</code> (see the <a href="https://software.intel.com/en-us/mkl-developer-reference-c-trtrs">intel docs</a>). I will look into that further
another time!
</p>
